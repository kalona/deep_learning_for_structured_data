{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# NYC Airbnb Price Prediction - Data Preparation\n\nUse dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n\n\nThis notebook contains the common data loading and preparation steps:\n- load data from the input CSV\n- fix missing values\n- clean up anomalies",
   "metadata": {
    "cell_id": "00000-c1a44fb8-56a3-4c82-bc7b-4ed08a8b0230",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Common imports and variables\nImports and variable definitions that are common to the entire notebook\n",
   "metadata": {
    "cell_id": "00001-24f85133-d361-465b-ad83-3dd921711bdd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-07ee31b9-e0d0-421d-ba28-23b4f46f48b8",
    "deepnote_cell_type": "code"
   },
   "source": "!pip install requests\n!pip install xlrd",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: requests in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.22.0)"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "You are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2019.6.16)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (1.25.3)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.0.4)\nRequirement already satisfied: xlrd in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.2.0)\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "You are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00003-17c1b639-ca53-4922-9c2b-f5455fac39dd",
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport datetime as dt\n# common imports\nimport zipfile\nimport time\n# import datetime, timedelta\nimport datetime\nfrom datetime import datetime, timedelta\nfrom datetime import date\nfrom dateutil import relativedelta\nfrom io import StringIO\nimport pandas as pd\nimport pickle\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom io import StringIO\nimport requests\nimport json\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport os\nimport math\nfrom subprocess import check_output\nfrom IPython.display import display\nimport logging\nimport yaml\nfrom collections import Counter\nimport re\nimport os\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-2ddff3e8-32d9-483d-8c16-1da5069c9fcc",
    "deepnote_cell_type": "code"
   },
   "source": "# load config file\ncurrent_path = os.getcwd()\nprint(\"current directory is: \"+current_path)\n\npath_to_yaml = os.path.join(current_path, 'airbnb_data_preparation_config.yml')\nprint(\"path_to_yaml \"+path_to_yaml)\ntry:\n    with open (path_to_yaml, 'r') as c_file:\n        config = yaml.safe_load(c_file)\nexcept Exception as e:\n    print('Error reading the config file')\n    ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "current directory is: C:\\personal\\manning\\second_example_june_2020\\airbnb\\dl_example_2\\notebooks\npath_to_yaml C:\\personal\\manning\\second_example_june_2020\\airbnb\\dl_example_2\\notebooks\\airbnb_data_preparation_config.yml\n"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-118f251e-b843-425d-9529-759bf52f318a",
    "deepnote_cell_type": "code"
   },
   "source": "# common variables\n# control whether to load data from scratch from original source or from saved dataframe\nload_from_scratch = config['general']['load_from_scratch']\n# control whether to save dataframe with transformed data\nsave_transformed_dataframe = config['general']['save_transformed_dataframe']\n# control whether rows containing erroneous values are removed from the saved dataset\nremove_bad_values = config['general']['remove_bad_values']\n# load default replacements for missing values\ntext_default = config['general']['text_default']\ncategorical_default = config['general']['categorical_default']\ntime_default = config['general']['time_default']\ncontinuous_default = config['general']['continuous_default']\n# original CSV version of input (unprocessed) dataset\ninput_csv = config['file_names']['input_csv']\n# saved pickled version of input dataset\npickled_input_dataframe = config['file_names']['pickled_input_dataframe']\n# name of file to which prepared data set is saved as a pickled dataframe\npickled_output_dataframe = config['file_names']['pickled_output_dataframe']\n# load lists of column categories\ncollist = config['categorical']\ntextcols = config['text']\ncontinuouscols = config['continuous']\nexcludefromcolist = config['excluded']\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-3a146b47-79c3-4c22-9eab-3af04f544965",
    "deepnote_cell_type": "code"
   },
   "source": "print(\"load_from_scratch \"+str(load_from_scratch))\nprint(\"save_transformed_dataframe \"+str(save_transformed_dataframe))\nprint(\"remove_bad_values \"+str(remove_bad_values))\nprint(\"pickled_input_dataframe \"+str(pickled_input_dataframe))\nprint(\"pickled_output_dataframe \"+str(pickled_output_dataframe))\nprint(\"defaults for text categorical time continuous are \"+text_default+\", \"+categorical_default+\", \"+str(time_default)+\", \"+str(continuous_default))\nprint(\"collist is: \",str(collist))\nprint(\"textcols is: \",str(textcols))\nprint(\"continuouscols is: \",str(continuouscols))\nprint(\"excludefromcolist is: \",str(excludefromcolist))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "load_from_scratch True\nsave_transformed_dataframe True\nremove_bad_values True\npickled_input_dataframe AB_NYC_2019_df.pkl\npickled_output_dataframe AB_NYC_2019_remove_bad_values_jun21_2020.pkl\ndefaults for text categorical time continuous are missing, missing, 2019-01-01, 0.0\ncollist is:  ['neighbourhood_group', 'neighbourhood', 'room_type']\ntextcols is:  ['name', 'host_name']\ncontinuouscols is:  ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count']\nexcludefromcolist is:  ['price', 'id', 'latitude', 'longitude', 'name', 'host_name', 'last_review']\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Load Data\n- ingest CVS into a Pandas dataframe ",
   "metadata": {
    "cell_id": "00007-6b3c8607-5187-4638-b4ab-614d11f181d2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-63498fc9-06f8-466f-a271-cd72c258bad6",
    "deepnote_cell_type": "code"
   },
   "source": "# get the directory for that this notebook is in and return the directory containing data files\n\ndef get_path():\n    rawpath = os.getcwd()\n    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n    return(path)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-36f55685-6f75-4cf9-bd11-f3c76767fb88",
    "deepnote_cell_type": "code"
   },
   "source": "# given a path return the list of xls files in the directory\ndef get_xls_list(path):\n    files = os.listdir(path)\n    files_xls = [f for f in files if f[-4:] == 'xlsx']\n    print(files)\n    print(files_xls)\n    return(files_xls)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-e724567e-a4ce-453b-a01d-3474d9751b8a",
    "deepnote_cell_type": "code"
   },
   "source": "# define categories for input columns\ndef define_feature_categories(df):\n    allcols = list(df)\n    print(\"all cols\",allcols)\n    textcols = ['name','host_name'] # \n    continuouscols = ['price','minimum_nights','number_of_reviews','reviews_per_month','calculated_host_listings_count','availability_365'] \n                      # columns to deal with as continuous values - no embeddings\n    timecols = ['last_review']\n    collist = ['neighbourhood_group','neighbourhood','room_type']\n    for col in continuouscols:\n        df[col] = df[col].astype(float)\n    print('texcols: ',textcols)\n    print('continuouscols: ',continuouscols)\n    print('timecols: ',timecols)\n    print('collist: ',collist)\n    return(allcols,textcols,continuouscols,timecols,collist)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-f7d37b34-2a49-48e9-9212-03ad5ec1f238",
    "deepnote_cell_type": "code"
   },
   "source": "# fill missing values according to the column category\ndef fill_missing(dataset,allcols,textcols,continuouscols,timecols,collist):\n    logging.debug(\"before mv\")\n    for col in collist:\n        dataset[col].fillna(value=categorical_default, inplace=True)\n    for col in continuouscols:\n        dataset[col].fillna(value=continuous_default,inplace=True)\n    for col in timecols:\n        dataset[col].fillna(value=time_default,inplace=True)\n    for col in textcols:\n        dataset[col].fillna(value=text_default, inplace=True)\n    return (dataset)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Load dataframe\n- load pickled dataframe\n- show info about the dataset\n",
   "metadata": {
    "cell_id": "00012-f1400925-faaa-42d4-bff5-6e4a2f2cfdbd",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-81ef1a0e-18e0-4caf-83b6-3a7379c36613",
    "deepnote_cell_type": "code"
   },
   "source": "# read in data, either from original CSV file in data directory or from saved pickled dataframe\ndef ingest_data(path):\n    if load_from_scratch:\n        unpickled_df = pd.read_csv(os.path.join(path,input_csv)) \n    else:\n        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n        logging.debug(\"reloader done\")\n    return(unpickled_df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# General cleanup\n- correct types for Route and Vehicle\n- fill missing values\n- create report-date-time index",
   "metadata": {
    "cell_id": "00014-851d30b6-09c9-4c8e-a62d-dd7c6d3f7cbe",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00015-9353f5fd-6b37-4731-8eb8-f0ad55d3b1e5",
    "deepnote_cell_type": "code"
   },
   "source": "# the dataset incorporated some anomalies in the 2019 data, including:\n# extraneous Incident ID in April 2019 tab\n# Gap and Delay columns in April and June 2019 tabs for what had otherwise been called Min Gap and Min Delay\n# this function cleans up these anomalies\ndef fix_anomalous_columns(df):\n    # for rows where there is NaN in the Min Delay or Min Gap columns, copy over value from Delay or Gap\n    # df.Temp_Rating.fillna(df.Farheit, inplace=True)\n    df['Min Delay'].fillna(df['Delay'], inplace=True)\n    df['Min Gap'].fillna(df['Gap'], inplace=True)\n    # now that the useful values have been copied from Delay and Gap, remove them\n    del df['Delay']\n    del df['Gap']\n    # remove Incident ID column - it's extraneous\n    del df['Incident ID']\n    return(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-1d53a1ce-af6d-48bb-8e9a-a8dbec9e63d0",
    "deepnote_cell_type": "code"
   },
   "source": "def replace_time(date_time_value,time_value):\n    ''' given a datetime replace the time portion '''\n     \n    date_time_value = date_time_value.replace(hour=time_value.hour,minute=time_value.minute,second=time_value.minute)\n    return(date_time_value)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-142c7d79-428e-493f-9515-5f369b925522",
    "deepnote_cell_type": "code"
   },
   "source": "def general_cleanup(df):\n    # ensure Route and Vehicle are strings, not numeric\n    df['Route'] = df['Route'].astype(str)\n    df['Vehicle'] = df['Vehicle'].astype(str)\n    # remove extraneous characters left from Vehicle values being floats\n    df['Vehicle'] = df['Vehicle'].str[:-2]\n    # tactical definition of categories\n    allcols,textcols,continuouscols,timecols,collist = define_feature_categories(df)\n    # fill in missing values\n    df.isnull().sum(axis = 0)\n    df = fix_anomalous_columns(df)\n    df = fill_missing(df,allcols,textcols,continuouscols,timecols,collist)\n    # create new column combining date + time (needed for resampling) and make it the index\n    df['Report Date Time'] = df.apply(lambda x: replace_time(x['Report Date'], x['Time']), axis=1)\n    df.index = df['Report Date Time']\n    # return the updated dataframe along with the column category lists\n    return(df,allcols,textcols,continuouscols,timecols,collist)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Clean up selected columns\nSome values in the input dataset were entered \"free form\" when they should have been constricted to a pick list. Columns with this problem include:\n\n- Route\n- Vehicle\n- Direction\n- Location\n\n\nEach of these have a finite set of valid values. We have to fix the data in these columns where multiple tokens have been used to signify the same real-world entity (e.g. \"roncesvalles yard.\" and \"roncesvalles carhouse\", or where incorrect values have been entered (e.g. Direction that does not correspond with a compass point)",
   "metadata": {
    "cell_id": "00018-fde80a06-ec83-426e-955f-d6b4d5aaadd7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Clean up Route",
   "metadata": {
    "cell_id": "00019-2365ceeb-8d62-4378-8192-ed92b47e10ac",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00020-53a39c9a-9b45-4451-b6d2-7629e63823a8",
    "deepnote_cell_type": "code"
   },
   "source": "def check_route (x):\n    if x in valid_routes:\n        return(x)\n    else:\n        return(\"bad route\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00021-73f2726e-7826-43df-97df-bd1e9efb4cdd",
    "deepnote_cell_type": "code"
   },
   "source": "def route_cleanup(df):\n    print(\"Route count pre cleanup\",df['Route'].nunique())\n    # df['Route'].value_counts()\n    # replace bad route with common token\n    df['Route'] = df['Route'].apply(lambda x:check_route(x))\n    print(\"route count post cleanup\",df['Route'].nunique())\n    return(df)    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Clean up Vehicle",
   "metadata": {
    "cell_id": "00022-8b9afe65-9990-4cca-8141-bbc84894c367",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00023-21f1576e-7254-4ac1-8a24-545d44c80327",
    "deepnote_cell_type": "code"
   },
   "source": "def check_vehicle (x):\n    if str.isdigit(x):\n        if int(x) in valid_vehicles:\n            return x\n        else:\n            return(\"bad vehicle\")\n    else:\n        return(\"bad vehicle\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-27abffab-84d8-48ec-9aa0-13f7ea787865",
    "deepnote_cell_type": "code"
   },
   "source": "def vehicle_cleanup(df):\n    print(\"Vehicle count pre cleanup\",df['Vehicle'].nunique())\n    df['Vehicle'] = df['Vehicle'].apply(lambda x:check_vehicle(x))\n    print(\"Vehicle count post cleanup\",df['Vehicle'].nunique())\n    return(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Clean up Direction",
   "metadata": {
    "cell_id": "00025-3a08c0af-76a9-456e-ba39-afbd8bc3d33b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-2577de7b-d14a-467f-bf7b-a6ce5c09acc8",
    "deepnote_cell_type": "code"
   },
   "source": "def check_direction (x):\n    if x in valid_directions:\n        return(x)\n    else:\n        return(\"bad direction\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-7c3c4504-30a2-4d89-a8d0-e110aa26569c",
    "deepnote_cell_type": "code"
   },
   "source": "def direction_cleanup(df):\n    print(\"Direction count pre cleanup\",df['Direction'].nunique())\n    df['Direction'] = df['Direction'].str.lower()\n    df['Direction'] = df['Direction'].str.replace('/','')\n    df['Direction'] = df['Direction'].replace({'eastbound':'e','westbound':'w','southbound':'s','northbound':'n'})\n    df['Direction'] = df['Direction'].replace('b','',regex=True)\n    df['Direction'] = df['Direction'].apply(lambda x:check_direction(x))\n    print(\"Direction count post cleanup\",df['Direction'].nunique())\n    return(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Clean up Location",
   "metadata": {
    "cell_id": "00028-f8c5837e-ca87-410b-8621-873f62a305c4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00029-2449894b-88dc-4bd1-805d-77cee99b8be2",
    "deepnote_cell_type": "code"
   },
   "source": "def clean_conjunction(intersection):\n    intersection = re.sub(\" *& *\",\" and \",intersection)\n    intersection = re.sub(\" */ *\",\" and \",intersection)\n    return(intersection)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00030-698f00b9-f7d0-4215-b75f-55bb2ede3316",
    "deepnote_cell_type": "code"
   },
   "source": "def order_location(intersection):\n    # for any string with the format \"* and *\" if the value before the and is alphabetically\n    # higher than the value after the and, swap the values\n    conj = \" and \"\n    alpha_ordered_intersection = intersection\n    if conj in intersection:\n        end_first_street = intersection.find(conj)\n        if (end_first_street > 0) and (len(intersection) > (end_first_street + len(conj))):\n            start_second_street = intersection.find(conj) + len(conj)\n            first_street = intersection[0:end_first_street]\n            second_street = intersection[start_second_street:]\n            alpha_ordered_intersection = min(first_street,second_street)+conj+max(first_street,second_street)\n    return(alpha_ordered_intersection)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-48e0d17f-f2ed-4fdc-8df2-78f75ab6c89b",
    "deepnote_cell_type": "code"
   },
   "source": "def location_cleanup(df):\n    print(\"Location count pre cleanup\",df['Location'].nunique())\n    # make all location values lower case\n    df['Location'] = df['Location'].str.lower()\n    # make substitutions to eliminate obvious duplicate tokens\n    df['Location'] = df['Location'].replace({'broadviewstation':'broadview station',' at ':' and ',' stn':' station',' ave.':'','/':' and ','roncy':'roncesvalles','carhouse':'yard','yard.':'yard','st. clair':'st clair','ronc. ':'roncesvalles ','long branch':'longbranch','garage':'yard','barns':'yard',' & ':' and '}, regex=True)\n    # put intersection values into consistent order\n    df['Location'] = df['Location'].apply(lambda x:order_location(x))\n    print(\"Location count post cleanup\",df['Location'].nunique())\n    return(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Remove bad rows",
   "metadata": {
    "cell_id": "00032-77e14df6-4b37-4e4a-ad0f-08e67c304cea",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-4e22cc77-8de1-4396-876b-58d6ad8a1779",
    "deepnote_cell_type": "code"
   },
   "source": "# remove rows with bad values\ndef remove_bad(df):\n    df = df[df.Vehicle != 'bad vehicle']\n    df = df[df.Direction != 'bad direction']\n    df = df[df.Route != 'bad route']\n    return(df)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Master cell\nThis cell contains calls to the other functions in this notebook to complete the data preparation",
   "metadata": {
    "cell_id": "00034-4ddd25b4-2973-4ee9-a9da-cd78b399e0c3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00035-2ff644a8-d27a-45db-a12d-f50904e964fd",
    "deepnote_cell_type": "code"
   },
   "source": "# master cell to call the other functions\n# get the path for data files\npath = get_path()\nprint(\"path is \",path)\n# load route direction and delay data datframes\ndf = ingest_data(path)\nallcols,textcols,continuouscols,timecols,collist = define_feature_categories(df)\n# iterate through columns to get basic information\nfor col in list(df):\n    print(\"Missing values in \",col,\" \",str(df[col].isna().sum()))\n    print(\"Distinct values \",str(df[col].nunique()))\ndf = fill_missing(df,allcols,textcols,continuouscols,timecols,collist)\ndf.head()\n'''\nprint(\"number of records: \",len(df.index))\nprint(\"df.info() output\",df.info())\nprint(\"df.shape output\",df.shape)\nprint(\"df.describe() output\",df.describe())\nprint(\"df.types output\",df.dtypes)\ndf,allcols,textcols,continuouscols,timecols,collist = general_cleanup(df)\ndf.head()\n# get record count by year\nfrom collections import Counter\ndf_year = pd.DatetimeIndex(df['Report Date Time']).year\nprint(\"record count by year pre processing: \", str(Counter(df_year)))\n# check that the values for April 2019 are correct\ndf[df['Report Date Time'].astype(str).str[:7]=='2019-04']\n# cleanup Route\nlogging.debug(\"df.shape output pre route cleanup\",df.shape)\ndf = route_cleanup(df) \ndf = vehicle_cleanup(df)\ndf = direction_cleanup(df)\ndf = location_cleanup(df)\nlogging.debug(\"df.shape output post location\",df.shape)\nprint(\"Bad route count pre:\",df[df.Route == 'bad route'].shape[0])\nprint(\"Bad direction count pre:\",df[df.Direction == 'bad direction'].shape[0])\nprint(\"Bad vehicle count pre:\",df[df.Vehicle == 'bad vehicle'].shape[0])\nif remove_bad_values:\n    df = remove_bad(df)\nprint(\"Bad route count:\",df[df.Route == 'bad route'].shape[0])\nprint(\"Bad direction count:\",df[df.Direction == 'bad direction'].shape[0])\nprint(\"Bad vehicle count:\",df[df.Vehicle == 'bad vehicle'].shape[0])\n# pickle the cleansed dataframe\nprint(\"df.shape output post removal of bad records \",df.shape)\n'''\nif save_transformed_dataframe:\n    print(\"path is \",path)\n    file_name = os.path.join(path,pickled_output_dataframe)\n    print(\"file_name is \",file_name)\n    df.to_pickle(file_name)\ndf.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00036-1bcd79d0-8457-46b4-8b58-d1fa5aa2787c",
    "deepnote_cell_type": "code"
   },
   "source": "df.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e50b8b52-ad76-402e-97f9-885942610c33' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "deepnote_notebook_id": "1c6b081c-9035-4c88-aa2f-6e5bc4c3676f",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}